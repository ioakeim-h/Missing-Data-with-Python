{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Standardizing Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Searching for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and missing data go hand-in-hand. Unidentified missing values will persist unnoticed and subsequently lead to issues during later stages of the analysis. These values could either be invalid, such as a value of 15 in an integer column with a valid range of 1 to 10, or they might be formatted in a way that doesn't appropriately signal their absence.\n",
    "\n",
    "Handling \"out of range\" values in a numeric series is a straightforward process. All that's required is to obtain the minimum and maximum values of the column and then replace any values that fall outside this valid range with a designated missing data indicator, such as `np.nan`.\n",
    "\n",
    "Identifying improperly formatted missing values can be more challenging, especially when dealing with columns that contain strings. Missing values formatted as strings can manifest in various forms (e.g., `\"-999\"`, `\"--\"`). Dealing with such cases often requires creative approaches or versatile tools like regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a research project focused on comparing salaries across European countries. Data were gathered using an online questionnaire, making it accessible to a wide range of individuals. The eligibility criteria encompassed being an employed adult residing in a European country. \n",
    "\n",
    "Our task is to explore the \"Age\" and \"Country\" variables for any inconsistencies and address them. The csv file for this task can be found in the notes of this video. Load it up and let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Country</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>30635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>29612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>97246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>40195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>Germany</td>\n",
       "      <td>87278.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age         Country   Salary\n",
       "0   26          Sweden  30635.0\n",
       "1   66          Cyprus  29612.0\n",
       "2   66  United Kingdom  97246.0\n",
       "3   24         Andorra  40195.0\n",
       "4   51         Germany  87278.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Data/EuroSalaries.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the research project focuses solely on adults, any age values below 18 are considered invalid. We should also set an upper limit such as 100 or 120, considering that life expectancy rarely extends beyond that range.\n",
    "\n",
    "Subsequently, it becomes essential to devise a strategy for handling out-of-range age values. These anomalies may arise from data entry errors or technical glitches, but they might also represent genuine entries. If we identify them as errors, marking them as missing data is a viable solution. Conversely, if these values are genuine, we should consider removing the entire row from the dataset. This is because individuals falling beyond the specified age range are outside the study's intended focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum age: 19\n",
      "Maximum age: 170\n",
      "3049    125\n",
      "3047    162\n",
      "2111    151\n",
      "224     144\n",
      "1122    162\n",
      "Name: Age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get min and max age\n",
    "print(\"Minimum age:\", df[\"Age\"].min())\n",
    "print(\"Maximum age:\", df[\"Age\"].max())\n",
    "\n",
    "# Display a sample of ages above 100 yrs old\n",
    "print(\n",
    "    df.loc[df[\"Age\"] > 100, \"Age\"].sample(5)\n",
    ")\n",
    "\n",
    "# Mark invalid ages as missing\n",
    "df.loc[df[\"Age\"] > 100, \"Age\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very unlikely that values above 100 are genuine and that is why they are marked as missing.\n",
    "\n",
    "Now that we have made the necessary adjustments to the \"Age\" column, we should also have a look at its data type. This will let us know of the values it can contain so we should expect a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Age\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being float is actually what we would expect after introducing `np.nan` into the column. `np.nan` is underneath a floating-point value and will cause an integer column to convert into float. We will see how to deal with this issue in part 2 of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research project has the goal of comparing salaries among European countries. Consequently, the \"Country\" column should exclusively contain entries of European countries. When dealing with categorical variables like this, there are two approaches to identify problematic data:\n",
    "\n",
    "The first approach involves utilizing the .unique() method to display all countries within the column. However, this method can be challenging to implement if the variable contains an extensive list of categories.\n",
    "\n",
    "The second approach is to validate entries in the \"Country\" column by comparing them to a pre-established list of legitimate European countries. Such a list can be sourced externally from reliable sources such as Google or by consulting resources like chatGPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Canada', 'India', '--'}\n"
     ]
    }
   ],
   "source": [
    "# Option A: Manually identify errors\n",
    "df[\"Country\"].unique()\n",
    "\n",
    "# Option B: Compare values in Country to a list of valid values (generated by chatGPT)\n",
    "european_countries = [\"Albania\", \"Andorra\", \"Austria\", \"Belarus\", \"Belgium\", \"Bosnia and Herzegovina\", \n",
    "                      \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Czech Republic\", \"Denmark\", \"Estonia\", \"Finland\", \n",
    "                      \"France\", \"Germany\", \"Greece\", \"Hungary\", \"Iceland\", \"Ireland\", \"Italy\", \"Kosovo\", \n",
    "                      \"Latvia\", \"Liechtenstein\", \"Lithuania\", \"Luxembourg\", \"Malta\", \"Moldova\", \"Monaco\", \n",
    "                      \"Montenegro\", \"Netherlands\", \"North Macedonia\", \"Norway\", \"Poland\", \"Portugal\", \n",
    "                      \"Romania\", \"Russia\", \"San Marino\", \"Serbia\", \"Slovakia\", \"Slovenia\", \"Spain\", \"Sweden\", \n",
    "                      \"Switzerland\", \"Ukraine\", \"United Kingdom\", \"Vatican City\"]\n",
    "\n",
    "## Unlike lists, sets contain only unique values - handy when printing data\n",
    "potential_errors = set()\n",
    "for country in df[\"Country\"].to_numpy():\n",
    "    if country not in european_countries:\n",
    "        potential_errors.add(country)\n",
    "print(potential_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two countries that are not located in Europe, namely India and Canada, were identified within the \"Country\" column. Additionally, an entry with an invalid value was found.\n",
    "\n",
    "To ensure the accuracy and focus of our study, we should proceed by eliminating rows containing non-European countries from the dataset. Their data does not align with the scope of our investigation.\n",
    "\n",
    "Concerning the entry with an invalid value, it's recommended to flag that specific instance as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the value in the 'Country' column is 'India' or 'Canada'\n",
    "countries_to_drop = ['India', 'Canada']\n",
    "df = df[~df['Country'].isin(countries_to_drop)]\n",
    "\n",
    "# Replace '--' with np.nan to indicate missingness\n",
    "df[\"Country\"] = df[\"Country\"].replace(\"--\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data type check is not necessary for the \"Country\" column. Unlike \"Age\", which contains only numerical values, the \"Country\" column exclusively holds strings and is thus classified as an \"object\" column type. Instead of focusing on data types, it would be more beneficial to concentrate on identifying non-string values. This is because an \"object\" column can accommodate values of various data types.\n",
    "\n",
    "However, we have already addressed this while searching for errors in the column. If there was a value outside of the `european_countries` list present within the column, it would have been displayed in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's advisable to thoroughly understand your data before carrying out any operations on it. In the context of this course, we've swiftly explored error identification and correction. However, when dealing with real-world data, it's essential to dedicate more time to comprehending the diverse column data types as this understanding guides us in determining the appropriate operations to apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Missing Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essential that the format of missing data being used can be recognized by our chosen data analysis library, which in our case is pandas. Popular examples include:\n",
    "\n",
    "- <strong>np.nan</strong> (NaN): This is the representation of a floating-point \"Not-a-Number\" value in NumPy. It's commonly used to represent missing or undefined values in numeric arrays. \n",
    "  \n",
    "- <strong>pd.NaT</strong> (NaT): \"Not a Time\" is specific to datetime columns in pandas.\n",
    "  \n",
    "- <strong>None</strong>: This is a special object in Python that also represents missing values.\n",
    "\n",
    "np.nan was preferred for representing missing values in pandas due to its alignment with the NumPy library. Since pandas was developed on the foundation of NumPy, which had already established the convention of utilizing np.nan to denote missing or undefined numerical values, this choice became a natural fit.\n",
    "\n",
    "However, limitations arose in terms of type compatibility. We have already seen how the mere presence of np.nan led to the conversion of integer values into floats. Another issue occurs when attempting to convert all values to the string data type. This process results in missing values being changed into strings, thereby removing their original indicator that denoted their absence. To address these limitations and others, pandas v.1.0 introduced `pd.NA`:\n",
    "\n",
    "- <strong>pd.NA</strong> (\\<NA\\>): A specialized missing value marker that works with various data types.\n",
    "\n",
    "Unlike `np.nan` which is an actual value, `pd.NA` acts as a missing indicator which means that it does not affect column data types and other values, but rather it is ignored by most calculations. \n",
    "\n",
    "Let's dive into the EuroSalaries dataset to test it. The \"Salary\" column should be an integer, but it's currently a float due to missing values formatted as np.nan. We'll see if replacing np.nan with pd.NA changes the column's data type to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Salary\"] = df[\"Salary\"].replace(np.nan, pd.NA)\n",
    "df[\"Salary\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type did change, but not as anticipated. Instead of becoming an integer, it shifted to an object type. Within standard pandas data types, pd.NA doesn't yield the expected results and shares comparable limitations with using np.nan.  In version 1.0 of pandas, they also introduced nullable data types that are capable of accommodating missing values effectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Nullable Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nullable data types can accommodate missing values without altering the state of the data. They were designed to improve memory use and performance while offering a user-friendly approach to manage missing data more flexibly. \n",
    "\n",
    "\n",
    "| Standard Pandas Dtype | Corresponding Nullable Dtype|\n",
    "|------------------|-----------------------------|\n",
    "| int64            | Int64                       |\n",
    "| float64          | Float64                     |\n",
    "| bool             | BooleanDtype                |\n",
    "| datetime64       | No direct nullable type     |\n",
    "| timedelta64      | No direct nullable type     |\n",
    "| object           | StringDtype                 |\n",
    "| category         | No direct nullable type     |\n",
    "\n",
    "Regular pandas data types can be converted to their nullable counterpart with the `.astype()` command:\n",
    "\n",
    "- Nullable integer: `df[col].astype(\"Int64\")` - note the capital `I`\n",
    "- Nullable float: `df[col].astype(\"Float64\")` - note the capital `F`\n",
    "- Nullable boolean: `df[col].astype(pd.BooleanDtype())` \n",
    "- Nullable string: `df[col].astype(pd.StringDtype())` or `df[col].astype(\"string\")`\n",
    "\n",
    "We can also automatically convert all columns with the `.convert_dtypes()` method. Let's see it in action with the EuroSalaries dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age         Int64\n",
       "Country    string\n",
       "Salary     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nullable = df.convert_dtypes()\n",
    "df_nullable.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns have transitioned to nullable data types. For instance, the \"Age\" column, previously holding float data due to np.nan values, now holds integers. This change occurred because, when converting columns to nullable, all missing value representations are unified to pd.NA. Since pd.NA functions well with various data types, the \"Age\" column appropriately assumed its true data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to learn more about conventional data types and the new additions, you can check out the notes associated with this video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>Video Notes</u>\n",
    "\n",
    "<h4>Pandas Data Types</h4>\n",
    "\n",
    "Pandas provides various data structures, including different data types for handling different types of data. Here is a list of pandas data types along with a brief description for each:\n",
    "\n",
    "- object: This allows for mixed data types within a column although it is often used for string data.\n",
    "\n",
    "- int8, int16, int32, int64: These are integer data types and are used for columns that contain whole numbers.\n",
    "\n",
    "- float16, float32, float64: These are floating-point data types and are used for columns that contain decimal numbers.\n",
    " \n",
    "- bool: This represents a boolean data type, which can have values True or False. It's often used for columns that contain binary or categorical data.\n",
    "\n",
    "- datetime64: This represents date and time values with nanosecond precision. It's used for columns that store timestamp information.\n",
    "\n",
    "- timedelta64: This represents differences between two datetime64 values. It's used for columns that store durations or time intervals.\n",
    "\n",
    "- category: This represents categorical data. It's used to store data with a limited and fixed set of values.\n",
    "\n",
    "<br>\n",
    "New additions:\n",
    "\n",
    "- StringDtype: This is an extension of the object data type for columns that contain only strings. It supports both strings and missing values.\n",
    "  \n",
    "- Int8, Int16, Int32, Int64: These are nullable integer data types that can support both integers and missing values.\n",
    "\n",
    "- UInt8, UInt16, UInt32, UInt64: These are nullable unsigned integer data types. They're used for columns where only positive values are needed and memory usage is a concern.\n",
    "\n",
    "- Float16, Float32, Float64: These are nullable floating-point data types that can support both floats and missing values.\n",
    "\n",
    "- PeriodDtype: This is used for representing regular intervals of time, such as days, months, or years.\n",
    "\n",
    "- BooleanDtype: This is a nullable boolean data type. It's introduced to address scenarios where you want to represent missing or unknown boolean values in your data.\n",
    "\n",
    "- CategoricalDtype: Similar to the category data type, this is used for representing categorical data, but it allows for custom ordering of categories.\n",
    "\n",
    "- SparseDtype: This is used to represent sparse data structures, which efficiently store data with a lot of missing values.\n",
    "\n",
    "<h4>Memory Efficiency and Computational Speed</h4>\n",
    "\n",
    "You might have noticed that data types like int or float offer various bit sizes (e.g. `int8`, `int16`, `int32`, `int64`). These choices are designed to cater to diverse needs related to memory capacity and computation speed. Smaller bit sizes in data types mean they are less precise, but they consume less memory and lead to quicker calculations. Precision refers to how accurately a number can be represented. Just like a ruler with more marks can measure more precisely, higher precision data types can capture finer details in numbers.\n",
    "\n",
    "For instance, when we convert a column to the `int` type using a command like `.astype(int)`, the default allocation is 64 bits. This allows the column to hold very large whole numbers with a high level of accuracy. However, in some situations, such as when working with smaller numbers, we might not need such high precision. In such cases, opting for a smaller data type like `int32` could be more memory-efficient without sacrificing accuracy.\n",
    "\n",
    "In essence, allocating more bits leads to greater precision but lower efficiency in terms of memory and speed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to handle invalid entries and explored efficient ways of formatting data. To practice what we have learned, we will use the data of 30000 runners. Our goal is to spot and rectify incorrect values, ensure consistent column data types, and ensure that all missing values can be retrieved with the .isna() method. \n",
    "\n",
    "The exercise requires a few coding procedures that were not covered in the course. These were intended to encourage you to use the web as you would do in a real-world setting. If you find yourself struggling, you can check out the hints in the notes of this video.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Hints\n",
    "\n",
    "- The dataset has a total of 5131 missing values\n",
    "- Regular expression site for testing and cheatsheet: https://pythex.org/\n",
    "\n",
    "### Column-specific Hints\n",
    "\n",
    "- In the \"Experience\" column the are two invalid instances. These can be obtained with the `.unique()` method and adjusted to your liking with `.replace()`. \n",
    "\n",
    "- \"Sleep Duration (hours)\", \"Distance Covered (km)\"\tand \"Average Pace (min/km)\" have invalid values. These can be located with the `str.contains()` method along with a regular expression like `[^\\d\\.]+`. In cases where `.replace()` does not work as expected, try using `str.replace()` instead.\n",
    "\n",
    "- The column \"Average Heart Rate (bpm)\" is already in numeric format but includes values that are out of range. Heart rate values above 220 are too extreme for human physiology. You may consider replacing these with `pd.NA` by following this template: `df.loc[df[condition, \"column\"] = value_to_assign`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through the exercise together. If you didn't manage to get the expected results then don't worry. Remember that working with data requires a lot of patience and practice.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial step is to display a sample of the data and get an understanding of how the majority of values within each column look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experience</th>\n",
       "      <th>Sleep Duration (hours)</th>\n",
       "      <th>Distance Covered (km)</th>\n",
       "      <th>Average Pace (min/km)</th>\n",
       "      <th>Average Heart Rate (bpm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advanced</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.02</td>\n",
       "      <td>4.59</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beginner</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>6.9</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advanced</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.57</td>\n",
       "      <td>4.73</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advanced</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.27</td>\n",
       "      <td>3.3</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beginner</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.66</td>\n",
       "      <td>6.02</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Experience Sleep Duration (hours) Distance Covered (km)  \\\n",
       "0   advanced                    7.0                 12.02   \n",
       "1   beginner                    6.0                  3.19   \n",
       "2   advanced                    8.0                 19.57   \n",
       "3   advanced                    7.0                 10.27   \n",
       "4   beginner                    6.0                  7.66   \n",
       "\n",
       "  Average Pace (min/km)  Average Heart Rate (bpm)  \n",
       "0                  4.59                       119  \n",
       "1                   6.9                       200  \n",
       "2                  4.73                       112  \n",
       "3                   3.3                       117  \n",
       "4                  6.02                       180  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Data/running_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then like to examine data types and check if the values in columns match the expected data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience                  object\n",
       "Sleep Duration (hours)      object\n",
       "Distance Covered (km)       object\n",
       "Average Pace (min/km)       object\n",
       "Average Heart Rate (bpm)     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these, I can already tell that most of the columns have problematic values. For example, columns like sleep duration, distance covered, and average pace contain floats, yet their data type is listed as object. This means that other data types could exist within them. \n",
    "\n",
    "In contrast, experience and average heart rate are not so obvious because they hold a reasonable data type according to their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is probably a good point to convert columns to their nullable data type, so I wil just go ahead and do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience                  string\n",
       "Sleep Duration (hours)      string\n",
       "Distance Covered (km)       string\n",
       "Average Pace (min/km)       string\n",
       "Average Heart Rate (bpm)     Int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.convert_dtypes()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just take these columns one at a time and start with experience.\n",
    "Experience looks like a categorical variable with few categories. We may be able to locate errors by displaying all its unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StringArray>\n",
       "['advanced', 'beginner', 'intermediate', '   ', 'begin']\n",
       "Length: 5, dtype: string"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Experience\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the \"Experience\" category comprises three main labels: \"advanced,\" \"beginner,\" and \"intermediate\". However, we can see two additional values: an empty string and \"begin\". It's likely that \"begin\" stands for \"beginner\".\n",
    "\n",
    "To address this, we can treat the empty string as missing data and use a regular expression that matches white space to capture it. As for the value \"begin\", we can adjust it to \"beginner\" to align with the existing three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StringArray>\n",
       "['advanced', 'beginner', 'intermediate', <NA>]\n",
       "Length: 4, dtype: string"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Experience\"] = df[\"Experience\"].replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "df[\"Experience\"] = df[\"Experience\"].replace(\"begin\", \"beginner\")\n",
    "\n",
    "# Check\n",
    "df[\"Experience\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on. \n",
    "\n",
    "Sleep duration, distance covered, and average pace were initially labeled as type \"object,\" which led to their data type becoming \"string\" after the conversion. However, based on the extracted sample, they should have likely been represented as floats. This discrepancy suggests that values might have been mistakenly formatted as strings, or there might be a mix of different data types within these columns.\n",
    "\n",
    "We can start by looking for non-numeric values. The `str.contains()` method can be used to create a mask based on a regular expression pattern which in our case will capture non-numeric values. This mask will then allow us to display these values and decide if there are any issues in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "647      ?\n",
       "859      ?\n",
       "949      ?\n",
       "961      ?\n",
       "1171     ?\n",
       "        ..\n",
       "28393    ?\n",
       "28486    ?\n",
       "29456    ?\n",
       "29656    ?\n",
       "29830    ?\n",
       "Name: Sleep Duration (hours), Length: 66, dtype: string"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mask for non-numeric values using regex\n",
    "non_numeric_mask = df[\"Sleep Duration (hours)\"].str.contains(r\"[^\\d\\.]+\", regex=True)\n",
    "\n",
    "# Display values\n",
    "df.loc[non_numeric_mask, \"Sleep Duration (hours)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the problematic value is a question mark which is probably supposed to indicate missingness. Let's replace all occurences of this value with a valid missing indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sleep Duration (hours)\"] = df[\"Sleep Duration (hours)\"].replace(\"?\", pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no other data errors are present, the sleep duration column should now be able to be converted into a numeric type. Once this is done, we should have a look at the range of values to ensure no inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "df[\"Sleep Duration (hours)\"] = df[\"Sleep Duration (hours)\"].astype(\"Float64\")\n",
    "\n",
    "# Get min and max values\n",
    "print(df[\"Sleep Duration (hours)\"].min())\n",
    "print(df[\"Sleep Duration (hours)\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are the columns distance covered and average pace. These are similar in nature to sleep duration, so we can repeat the same procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9                      a17.9\n",
       "20                     a7.95\n",
       "27                    a18.81\n",
       "29       a16.490000000000002\n",
       "42                    a12.75\n",
       "                ...         \n",
       "29968                 a18.17\n",
       "29982                 a10.28\n",
       "29985                 a19.67\n",
       "29990                  a9.63\n",
       "29991                  a9.78\n",
       "Name: Distance Covered (km), Length: 3900, dtype: string"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numeric_mask = df[\"Distance Covered (km)\"].str.contains(r\"[^\\d\\.]+\", regex=True)\n",
    "df.loc[non_numeric_mask, \"Distance Covered (km)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, certain values are preceded by the letter \"a\". To revert them to their float type, we can simply remove the letter. This can be achieved by slicing and re-assigning the values after the \"a\" to their original positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[non_numeric_mask, \"Distance Covered (km)\"] = df[\"Distance Covered (km)\"].str[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the changes took place using an example with the value at index 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    17.9\n",
       "Name: Distance Covered (km), dtype: string"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with an example \n",
    "df.loc[df[\"Distance Covered (km)\"].index == 9, \"Distance Covered (km)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letter \"a\" is removed, meaning that the values are corrected. What remains is to convert the column to a numeric type and getting its range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "20.5\n"
     ]
    }
   ],
   "source": [
    "df[\"Distance Covered (km)\"] = df[\"Distance Covered (km)\"].astype(\"Float64\")\n",
    "print(df[\"Distance Covered (km)\"].min())\n",
    "print(df[\"Distance Covered (km)\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum and maximum values look reasonable so we can move on to the next column. \n",
    "The majority of values for average pace look like floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57       6,19\n",
       "112      6,76\n",
       "126      4,45\n",
       "319      5,17\n",
       "587      5,26\n",
       "         ... \n",
       "29179    5,78\n",
       "29516    5,06\n",
       "29548    3,94\n",
       "29550    5,47\n",
       "29594    6,98\n",
       "Name: Average Pace (min/km), Length: 303, dtype: string"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numeric_mask = df[\"Average Pace (min/km)\"].str.contains(r\"[^\\d\\.]+\", regex=True)\n",
    "df.loc[non_numeric_mask, \"Average Pace (min/km)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach as before, we discover that specific values are using a comma as a decimal separator. This can be easily resolved by substituting these commas with periods. However, using `.replace()` alone won't suffice here.\n",
    "\n",
    "In a previous instance, we employed the `.replace()` method to substitute question marks with `pd.NA` in the \"Sleep Duration\" column like this: `df[\"Sleep Duration (hours)\"].replace(\"?\", pd.NA)`. This approach worked as we were replacing entire strings with another value.\n",
    "\n",
    "In the current task, the comma acts as a substring within a larger string. To achieve the desired outcome, we must utilize the `str` accessor in conjunction with the `replace` method. This combination enables us to replace substrings with a different character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Average Pace (min/km)\"] = df[\"Average Pace (min/km)\"].str.replace(\",\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All occurences of a comma should now be replaced with a dot. If values are consistent we should now be able to convert to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' . '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/akis/Desktop/Missing Data/New/Module2.ipynb Cell 63\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akis/Desktop/Missing%20Data/New/Module2.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mAverage Pace (min/km)\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mAverage Pace (min/km)\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mFloat64\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5912\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5905\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   5906\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   5907\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   5908\u001b[0m     ]\n\u001b[1;32m   5910\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5911\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 5912\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   5913\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5915\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:419\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:304\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:580\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 580\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    582\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    583\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1292\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m   1291\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1292\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m   1293\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m   1294\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1234\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n\u001b[1;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(values, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m   1233\u001b[0m     \u001b[39m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m   1236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/string_.py:441\u001b[0m, in \u001b[0;36mStringArray.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    439\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misna()\n\u001b[1;32m    440\u001b[0m     arr[mask] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 441\u001b[0m     values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m.\u001b[39;49mnumpy_dtype)\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m FloatingArray(values, mask, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/string_.py:449\u001b[0m, in \u001b[0;36mStringArray.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    447\u001b[0m mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misna()\n\u001b[1;32m    448\u001b[0m arr[mask] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 449\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39;49mastype(dtype)\n\u001b[1;32m    450\u001b[0m values[mask] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m    451\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' . '"
     ]
    }
   ],
   "source": [
    "df[\"Average Pace (min/km)\"] = df[\"Average Pace (min/km)\"].astype(\"Float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion was not sucessful. The error informs us that a particular value `' . '` cannot be converted. We must find these invalid values and flag them as missing before attempting to convert the column again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_mask = df[\"Average Pace (min/km)\"].str.contains(r\"[^\\d\\.]+\", regex=True)\n",
    "\n",
    "# Flag invalids\n",
    "df.loc[non_numeric_mask, \"Average Pace (min/km)\"] = pd.NA\n",
    "\n",
    "# Convert to float\n",
    "df[\"Average Pace (min/km)\"] = df[\"Average Pace (min/km)\"].astype(\"Float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the minimum and maximum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Average Pace (min/km)\"].min())\n",
    "print(df[\"Average Pace (min/km)\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average pace is ready for analysis. The last column we should look at is average heart rate. This is correctly formatted as numeric so all we need to do is look for any values that do not make sense for a variable like heart rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "362\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Average Heart Rate (bpm)\"].min())\n",
    "print(df[\"Average Heart Rate (bpm)\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum value looks reasonable, but the maximum value is not something that a human can achieve. A quick search into google informs us that the maximum heart rate of any person can be roughly estimated by substracting age from 220. Therefore, we can be sure that any value above 220 is inaccurate. \n",
    "\n",
    "Such values can be marked as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "df.loc[df[\"Average Heart Rate (bpm)\"] > 220, \"Average Heart Rate (bpm)\"] = pd.NA\n",
    "\n",
    "# Check\n",
    "print(df[\"Average Heart Rate (bpm)\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum heart rate observed is now 218. We could argue that is too high but I would say it is within the realm of possibility when considering individual variations and situations of extreme stress.\n",
    "\n",
    "It's crucial to emphasize that establishing an upper limit beyond which values are deemed invalid is a distinct procedure from addressing outliers. Outliers represent values that are unusually extreme but still feasible within a given context. However, in our specific scenario, any heart rate exceeding 220 is considered invalid due to its inherent impossibility within the range of human physiological capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we need to do to have this data ready for analysis. All column data types have been standardized, values are accurate and all missing values are correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience                   292\n",
       "Sleep Duration (hours)       336\n",
       "Distance Covered (km)          0\n",
       "Average Pace (min/km)          3\n",
       "Average Heart Rate (bpm)    4500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See you in the next video!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
